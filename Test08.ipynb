{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cca6266",
   "metadata": {},
   "source": [
    "### GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe796740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "143de788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *block(100, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, 28*28),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        img = self.model(x)\n",
    "        img = img.view(img.size(0), 1, 28, 28)\n",
    "        return img\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, img):\n",
    "        x = self.model(img.view(img.size(0),-1))\n",
    "        return x\n",
    " \n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6dec677e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCELoss()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = nn.BCELoss()\n",
    "generator.cuda()\n",
    "discriminator.cuda()\n",
    "loss_func.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "feb84db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aed6fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        './data', download=False,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize((28, 28)),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize([0.5], [0.5])\n",
    "                                \n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79dd2915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/20, batch 0/469, [D loss = 0.566685] [G loss = 1.257586] \n",
      "epoch 0/20, batch 100/469, [D loss = 0.683367] [G loss = 0.528397] \n",
      "epoch 0/20, batch 200/469, [D loss = 0.548980] [G loss = 1.035666] \n",
      "epoch 0/20, batch 300/469, [D loss = 0.506490] [G loss = 0.904833] \n",
      "epoch 0/20, batch 400/469, [D loss = 0.577181] [G loss = 1.186145] \n",
      "epoch 1/20, batch 0/469, [D loss = 0.501223] [G loss = 1.119981] \n",
      "epoch 1/20, batch 100/469, [D loss = 0.578340] [G loss = 1.339372] \n",
      "epoch 1/20, batch 200/469, [D loss = 0.528464] [G loss = 0.939053] \n",
      "epoch 1/20, batch 300/469, [D loss = 0.570555] [G loss = 0.753809] \n",
      "epoch 1/20, batch 400/469, [D loss = 0.599229] [G loss = 0.844490] \n",
      "epoch 2/20, batch 0/469, [D loss = 0.592245] [G loss = 1.086094] \n",
      "epoch 2/20, batch 100/469, [D loss = 0.578085] [G loss = 1.150848] \n",
      "epoch 2/20, batch 200/469, [D loss = 0.543119] [G loss = 1.184411] \n",
      "epoch 2/20, batch 300/469, [D loss = 0.595494] [G loss = 0.692129] \n",
      "epoch 2/20, batch 400/469, [D loss = 0.587552] [G loss = 1.137918] \n",
      "epoch 3/20, batch 0/469, [D loss = 0.619960] [G loss = 1.501527] \n",
      "epoch 3/20, batch 100/469, [D loss = 0.581101] [G loss = 1.433327] \n",
      "epoch 3/20, batch 200/469, [D loss = 0.557980] [G loss = 1.190330] \n",
      "epoch 3/20, batch 300/469, [D loss = 0.566146] [G loss = 1.104059] \n",
      "epoch 3/20, batch 400/469, [D loss = 0.643552] [G loss = 0.547120] \n",
      "epoch 4/20, batch 0/469, [D loss = 0.545402] [G loss = 1.051855] \n",
      "epoch 4/20, batch 100/469, [D loss = 0.657883] [G loss = 0.675496] \n",
      "epoch 4/20, batch 200/469, [D loss = 0.583935] [G loss = 0.881684] \n",
      "epoch 4/20, batch 300/469, [D loss = 0.608910] [G loss = 0.754475] \n",
      "epoch 4/20, batch 400/469, [D loss = 0.580439] [G loss = 1.256037] \n",
      "epoch 5/20, batch 0/469, [D loss = 0.560211] [G loss = 1.072300] \n",
      "epoch 5/20, batch 100/469, [D loss = 0.571257] [G loss = 1.046432] \n",
      "epoch 5/20, batch 200/469, [D loss = 0.598372] [G loss = 0.814074] \n",
      "epoch 5/20, batch 300/469, [D loss = 0.940273] [G loss = 0.238178] \n",
      "epoch 5/20, batch 400/469, [D loss = 0.615263] [G loss = 0.841305] \n",
      "epoch 6/20, batch 0/469, [D loss = 0.689229] [G loss = 0.578323] \n",
      "epoch 6/20, batch 100/469, [D loss = 0.593053] [G loss = 1.462402] \n",
      "epoch 6/20, batch 200/469, [D loss = 0.614491] [G loss = 1.250640] \n",
      "epoch 6/20, batch 300/469, [D loss = 0.625850] [G loss = 0.818712] \n",
      "epoch 6/20, batch 400/469, [D loss = 0.582550] [G loss = 1.215150] \n",
      "epoch 7/20, batch 0/469, [D loss = 0.598203] [G loss = 0.887214] \n",
      "epoch 7/20, batch 100/469, [D loss = 0.579646] [G loss = 0.997389] \n",
      "epoch 7/20, batch 200/469, [D loss = 0.603790] [G loss = 1.297808] \n",
      "epoch 7/20, batch 300/469, [D loss = 0.626606] [G loss = 1.184529] \n",
      "epoch 7/20, batch 400/469, [D loss = 0.664744] [G loss = 1.569277] \n",
      "epoch 8/20, batch 0/469, [D loss = 0.579544] [G loss = 0.894814] \n",
      "epoch 8/20, batch 100/469, [D loss = 0.574513] [G loss = 1.411207] \n",
      "epoch 8/20, batch 200/469, [D loss = 0.571141] [G loss = 1.027987] \n",
      "epoch 8/20, batch 300/469, [D loss = 0.613254] [G loss = 0.717141] \n",
      "epoch 8/20, batch 400/469, [D loss = 0.667565] [G loss = 1.614319] \n",
      "epoch 9/20, batch 0/469, [D loss = 0.594230] [G loss = 1.065163] \n",
      "epoch 9/20, batch 100/469, [D loss = 0.647377] [G loss = 1.294952] \n",
      "epoch 9/20, batch 200/469, [D loss = 0.536700] [G loss = 0.993438] \n",
      "epoch 9/20, batch 300/469, [D loss = 0.571860] [G loss = 1.019755] \n",
      "epoch 9/20, batch 400/469, [D loss = 0.606050] [G loss = 1.142333] \n",
      "epoch 10/20, batch 0/469, [D loss = 0.570338] [G loss = 1.107995] \n",
      "epoch 10/20, batch 100/469, [D loss = 0.620026] [G loss = 0.691021] \n",
      "epoch 10/20, batch 200/469, [D loss = 0.622551] [G loss = 0.623301] \n",
      "epoch 10/20, batch 300/469, [D loss = 0.574170] [G loss = 1.048730] \n",
      "epoch 10/20, batch 400/469, [D loss = 0.539413] [G loss = 1.093825] \n",
      "epoch 11/20, batch 0/469, [D loss = 0.582507] [G loss = 0.865728] \n",
      "epoch 11/20, batch 100/469, [D loss = 0.582948] [G loss = 1.147476] \n",
      "epoch 11/20, batch 200/469, [D loss = 0.567867] [G loss = 1.141411] \n",
      "epoch 11/20, batch 300/469, [D loss = 0.584804] [G loss = 1.264474] \n",
      "epoch 11/20, batch 400/469, [D loss = 0.541290] [G loss = 1.295332] \n",
      "epoch 12/20, batch 0/469, [D loss = 0.609206] [G loss = 1.122226] \n",
      "epoch 12/20, batch 100/469, [D loss = 0.594416] [G loss = 1.098099] \n",
      "epoch 12/20, batch 200/469, [D loss = 0.547278] [G loss = 1.202303] \n",
      "epoch 12/20, batch 300/469, [D loss = 0.565544] [G loss = 1.021951] \n",
      "epoch 12/20, batch 400/469, [D loss = 0.541381] [G loss = 0.951115] \n",
      "epoch 13/20, batch 0/469, [D loss = 0.606755] [G loss = 0.982658] \n",
      "epoch 13/20, batch 100/469, [D loss = 0.592297] [G loss = 0.915171] \n",
      "epoch 13/20, batch 200/469, [D loss = 0.573425] [G loss = 1.142973] \n",
      "epoch 13/20, batch 300/469, [D loss = 0.595998] [G loss = 1.026757] \n",
      "epoch 13/20, batch 400/469, [D loss = 0.596386] [G loss = 0.849255] \n",
      "epoch 14/20, batch 0/469, [D loss = 0.590037] [G loss = 1.267802] \n",
      "epoch 14/20, batch 100/469, [D loss = 0.619022] [G loss = 1.246248] \n",
      "epoch 14/20, batch 200/469, [D loss = 0.626158] [G loss = 1.063565] \n",
      "epoch 14/20, batch 300/469, [D loss = 0.640843] [G loss = 0.634530] \n",
      "epoch 14/20, batch 400/469, [D loss = 0.604998] [G loss = 1.375325] \n",
      "epoch 15/20, batch 0/469, [D loss = 0.563224] [G loss = 1.009637] \n",
      "epoch 15/20, batch 100/469, [D loss = 0.583171] [G loss = 0.795249] \n",
      "epoch 15/20, batch 200/469, [D loss = 0.601289] [G loss = 1.022932] \n",
      "epoch 15/20, batch 300/469, [D loss = 0.600461] [G loss = 0.939452] \n",
      "epoch 15/20, batch 400/469, [D loss = 0.558741] [G loss = 0.952503] \n",
      "epoch 16/20, batch 0/469, [D loss = 0.576962] [G loss = 1.114116] \n",
      "epoch 16/20, batch 100/469, [D loss = 0.568338] [G loss = 1.216981] \n",
      "epoch 16/20, batch 200/469, [D loss = 0.623871] [G loss = 1.056703] \n",
      "epoch 16/20, batch 300/469, [D loss = 0.615784] [G loss = 1.039879] \n",
      "epoch 16/20, batch 400/469, [D loss = 0.599089] [G loss = 1.100950] \n",
      "epoch 17/20, batch 0/469, [D loss = 0.616748] [G loss = 0.849265] \n",
      "epoch 17/20, batch 100/469, [D loss = 0.579326] [G loss = 0.867112] \n",
      "epoch 17/20, batch 200/469, [D loss = 0.734697] [G loss = 1.414490] \n",
      "epoch 17/20, batch 300/469, [D loss = 0.605414] [G loss = 1.207975] \n",
      "epoch 17/20, batch 400/469, [D loss = 0.585172] [G loss = 0.922273] \n",
      "epoch 18/20, batch 0/469, [D loss = 0.566132] [G loss = 0.983209] \n",
      "epoch 18/20, batch 100/469, [D loss = 0.585517] [G loss = 0.973068] \n",
      "epoch 18/20, batch 200/469, [D loss = 0.558034] [G loss = 1.194251] \n",
      "epoch 18/20, batch 300/469, [D loss = 0.611819] [G loss = 1.140884] \n",
      "epoch 18/20, batch 400/469, [D loss = 0.577089] [G loss = 0.860840] \n",
      "epoch 19/20, batch 0/469, [D loss = 0.599307] [G loss = 0.747646] \n",
      "epoch 19/20, batch 100/469, [D loss = 0.646242] [G loss = 1.116253] \n",
      "epoch 19/20, batch 200/469, [D loss = 0.585056] [G loss = 1.168612] \n",
      "epoch 19/20, batch 300/469, [D loss = 0.616134] [G loss = 1.003041] \n",
      "epoch 19/20, batch 400/469, [D loss = 0.604966] [G loss = 0.995510] \n"
     ]
    }
   ],
   "source": [
    "epoches = 20\n",
    "generator.parameters()\n",
    "if not os.path.exists('./images'):\n",
    "    os.mkdir('./images')\n",
    "from torchvision.utils import save_image\n",
    "import time\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    for i, (imgs, _) in enumerate(dl):\n",
    "        valid = Variable(torch.cuda.FloatTensor(imgs.size(0),1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(torch.cuda.FloatTensor(imgs.size(0),1).fill_(0.0), requires_grad=False)\n",
    "        \n",
    "        real_imgs = Variable(imgs.type(torch.cuda.FloatTensor))  # ??\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        z = Variable(torch.cuda.FloatTensor(np.random.normal(0, 1, (imgs.shape[0], 100) ) ) )\n",
    "        \n",
    "        gen_imgs = generator(z)\n",
    "        \n",
    "        g_loss = loss_func(discriminator(gen_imgs) , valid)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        ##\n",
    "        optimizer_D.zero_grad()\n",
    "        r_loss = loss_func(discriminator(real_imgs), valid)\n",
    "        f_loss = loss_func(discriminator(gen_imgs.detach()), fake)\n",
    "        \n",
    "        d_loss = (r_loss + f_loss) /2 ## using torch add_ ??\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # print\n",
    "        if i % 100 == 0:\n",
    "            print(\"epoch %d/%d, batch %d/%d, [D loss = %f] [G loss = %f] \" %(\n",
    "                (epoch, epoches, i, len(dl), d_loss.item(), g_loss.item())\n",
    "            ))\n",
    "            #\n",
    "            save_image(gen_imgs[:25], \"./images/image_%01d_%04d_%s.png\"%(epoch, i, time.strftime('%m-%d_%H.%M', time.localtime())), nrow=5, normalize=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77a3b8e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH.\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m, time\u001b[38;5;241m.\u001b[39mlocaltime())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "time.strftime('%m-%d_%H.%M', time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08f516be",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
